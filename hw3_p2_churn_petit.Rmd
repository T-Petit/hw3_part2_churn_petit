---
title: "hw3_p2_churn"
output: html_document
date: "2024-03-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This problem is based on one of [Kaggle's Playground Series of competitions](https://www.kaggle.com/docs/competitions). The Playground Series is a nice way to practice building predictive models by "providing interesting and approachable datasets for our community to practice their machine learning skills". 

You do **NOT** need to download any data from Kaggle. I've created a smaller dataset with some other modifications for use in our HW problem. The datafile, `churn.csv`, is available in the `data` subfolder.

This particular [playground dataset involves data about bank customers](https://www.kaggle.com/competitions/playground-series-s4e1) with the target variable being a binary indicator of whether or not the customer left the bank (`Exited`), or "churned". The playground dataset was constructed using another [Kaggle dataset on bank customer churn prediction](https://www.kaggle.com/datasets/shubhammeshram579/bank-customer-churn-prediction). Follow the preceeding link for information about the variables in this dataset. 

This assignment will focus on building simple classification models for
predicting bank customer churn. You'll be doing your work right in this R Markdown document. Feel free to save it first with a modified filename that includes your name. For example, mine would be **hw3_p2_churn_isken.Rmd**.

You'll likely need a bunch of libraries. I've included a few here but you should add any others that you need. If you don't need some of these, feel free to delete such lines.

```{r}
library(dplyr)   # Group by analysis and other SQLish things.
library(ggplot2) # Plotting, of course
library(corrplot) # Correlation plots
library(caret)   # Many aspects of predictive modeling
library(skimr)  # An automated EDA tool 
library(rpart) # Used to build decision trees
library(rpart.plot) # Plotting decision trees
library(randomForest) # Create random forest model
```
**MAJOR (10%) HACKER EXTRA** Version control

Create a new R Project for this assignment. Put the project under version control with git. Create a private GitHub repository for this project. Use git and GitHub as you go to do commits periodically and push them to your remote repository. After you have completed the assignment and pushed your last commit to your GitHub repo, add me as a Collaborator (my GitHub username is misken) so that I can see your repo.

> Added misken as a collaborator. Link to repository: https://github.com/T-Petit/hw3_part2_churn_petit.git

I cover use of git and GitHub with R Studio in this module on our course web page:

* [http://www.sba.oakland.edu/faculty/isken/courses/mis5470_f23/git_intro.html](http://www.sba.oakland.edu/faculty/isken/courses/mis5470_f23/git_intro.html)

This Hacker Extra is worth 10% of the total number of points in the assignment.

## Step 1: Read in data

Read the `churn.csv` file from the `data` subfolder into a dataframe named `churn`.

```{r read_churn}
churn <- read.csv("./data/churn.csv")
```

Use `str`, `summary`, and `skim` to get a sense of the data. 
The binary target variable is `Exited` where 1 indicates that the customer left the bank (they "churned"). You'll notice that some of the fields are numeric and some are character data. You might also notice that there are fewer variables in our churn dataset than in the original Kaggle versions.

```{r churn_str}
str(churn)
```

```{r churn_summary}
summary(churn)
```

```{r churn_skim}
skim(churn)
```

## Step 2: Factor conversions

Some of the variables clearly should be factors. Change all of the variables to factors that you think should be. Include an explanation of why each of these variables should be converted to factors.

```{r factor_conversions}
churn$Geography <- as.factor(churn$Geography)
churn$Gender <- as.factor(churn$Gender)
churn$HasCrCard <- as.factor(churn$HasCrCard)
churn$IsActiveMember <- as.factor(churn$IsActiveMember)
churn$Exited <- as.factor(churn$Exited)
```

> I decided to convert both `Geography` and `Gender` into factors because these to variables make more sense as factors than characters. `Gender` has two distinct categories and `Geography` has three distinct categories and categorical data makes more sense as factors than as character data. I also decided to convert `HasCrCard`, `IsActiveMember`, and `Exited` since these are binary variables which are categorical and make more sense as Factors since measures such as mean don't make sense for these variables whereas counts make sense.

## Step 3 - Partition into training and test sets

We will use the [caret](https://topepo.github.io/caret/) package to do the partitioning of our data into training and test dataframes. Just run this chunk to create training and test datasets. This way we'll
all be working with the same datasets. Notice that the test set is 20% of the full dataset.

```{r partition}
# Simple partition into train (80%) and test (20%) set 
set.seed(687) # Do NOT change this
trainIndex <- createDataPartition(churn$Exited, p = .8, 
                                  list = FALSE, 
                                  times = 1)

churn_train <- churn[as.vector(trainIndex), ]  
churn_test <- churn[-as.vector(trainIndex), ]

```

Find the number of customers and the percentage of customers for the two `Exited` levels. You'll
see that there are about 20% of the bank customers exited.

```{r target_prop_check_train}
#Number of Customers
table(churn_train$Exited)
#Percentage of Customers
prop.table(table(churn_train$Exited)) * 100
```

## Step 4: EDA

Do some EDA to try to uncover some relationships that may end up being useful in building a predictive model for `Exited`. You learned things in HW2 which should be useful here. You should **ONLY** use `churn_train` for your EDA. You should explore all of the variables.

```{r}
summary(churn_train)
```
> Now that the binary variables are declared as Factors instead of Integers, we get a count of each when we run the summary function. Here we can see that most of the customers reside in France, are male, and have a credit card. We also can see that only half of the members are active. Finally, the majority of customers have not churned. 

```{r Salary_by_CreditScore}
ggplot(data = churn_train) +
  geom_point(aes(x = EstimatedSalary, y = CreditScore, colour = Exited))
```

> No clear relationship between `CreditScore` and `EstimatedSalary` or between these and the `Exited` variable. 

```{r boxplot_EstimatedSalary}
ggplot(data = churn_train) +
  geom_boxplot((aes(x = Exited, y = EstimatedSalary, colour = Exited)))
```

> Estimated Salary doesn't seem to impact whether or not a customer will Exit. 

```{r}
ggplot(data = churn_train) +
  geom_boxplot((aes(x = Exited, y = Balance, colour = Exited)))
```

> It appears that customers that churn have a much higher balance than customers that don't.

```{r boxplot_CreditScore}
ggplot(data = churn_train) +
  geom_boxplot((aes(x = Exited, y = CreditScore, colour = Exited)))
```

> Credit score doesn't seem to have a significant impact on whether or not the customer exited.

```{r boxplot_Tenure}
ggplot(data = churn_train) +
  geom_boxplot((aes(x = Exited, y = Tenure, colour = Exited)))
```

> Tenure also doesn't seem to have much of an affect on whether or not the customer exited.

```{r boxplot_Age}
ggplot(data = churn_train) +
  geom_boxplot((aes(x = Exited, y = Age, colour = Exited)))
```

> This boxplot reveals that on average, older customers churn more than younger customers.

```{r histograms}
ggplot(churn_train) + geom_histogram(aes(x = CreditScore))
ggplot(churn_train) + geom_histogram(aes(x = Age))
ggplot(churn_train) + geom_histogram(aes(x = Balance))
ggplot(churn_train) + geom_histogram(aes(x = EstimatedSalary))
```

> `CreditScore` is slightly left skewed with the majority of scores falling between 550 and 750. `Age` is right skewed with most customers being in their 30s and 40s. `Balance` has the majority at 0 with the rest normally distributed around 125,000. `EstimatedSalary` is slightly left skewed. 

```{r bar_Geography}
ggplot(churn_train) + geom_bar(aes(x = Geography, fill = Exited)) +
  ggtitle("Number of Customers by Geography") +
  xlab("Geography") +
  ylab("Number of Customers")
```

> Most customers reside in France (15,000). About 5,000 customers reside in Germany & Spain. No clear relationship between `Geogrpahy` and `Exited`.

```{r bar_Gender}
ggplot(churn_train) + geom_bar(aes(x = Gender, fill = Exited)) +
  ggtitle("Number of Customers by Gender") +
  xlab("Gender") +
  ylab("Number of Customers")
```

> There are about 30,000 more male customers than Female customers. Although there are more male customers, more female customers have churned. 

```{r bar_NumOfProducts}
ggplot(churn_train) + geom_bar(aes(x = NumOfProducts, fill = Exited)) +
  ggtitle("Number of Customers by Number of Products") +
  xlab("Number of Products") +
  ylab("Number of Customers")
```

> This bar chart reveals that most customers have 1 or 2 accounts; however, customers with 1 account have a much higher chance of churning. Additionally, it appears that a lot of customers with 3 products also churned.

```{r bar_HasCrCard}
ggplot(churn_train) + geom_bar(aes(x = HasCrCard, fill = Exited)) +
  ggtitle("Number of Customers by Credit Card") +
  xlab("Has Credit Card") +
  ylab("Number of Customers")
```

> A significantly larger number of customers have a credit card than not. It also seems that the proportion of customers who churned is comparable between those that have and do not have credit cards. 

```{r bar_IsActiveMember}
ggplot(churn_train) + geom_bar(aes(x = IsActiveMember, fill = Exited)) +
  ggtitle("Number of Customers by Activity") +
  xlab("Active (1) vs. Not Active (0)") +
  ylab("Number of Customers")
```

> The number of customers that are active versus not active are about the same; however, as expected, the members who are not active have a higher rate of exiting. 


## Step 5 - Building and evaluation of predictive classification models

Now that you know a little more about the data, it's time to start building a
few classification models for `Exited`. We will start out using overall prediction accuracy
as our metric but we might want to consider other metrics.

**QUESTION** Why might overall prediction accuracy not be the most appropriate metric to consider? What other
metrics might be important and why?

> Accuracy is not the most appropriate metric because it only considers the proportion of cases classified correctly. So, if the data is heavily imbalanced (say a 90/10 split), you could easily just classify all cases as the majority group and have a a high accuracy (90% in the case of a 90/10 split). It is better to use this metric along with other metrics such as sensitivity and specificity. These measures give the proportion of 1s classified as 1s and proportion of 0s classified as 0s respectively. This way, if the data is imbalanced and everything is just being classified as a 0, the specificity and accuracy would be high but the sensitivity would be 0%.

### Fit a null model

A very simple model would be to simply predict that `Exited` is equal to 0. On
the training data we saw that we'd be ~80% accurate.

Let's create this null model and run a confusion matrix on its "predictions" for both the training
and the test data.

```{r tree_null}
# Create a vector of 0's
model_train_null <- rep(0, nrow(churn_train))
model_test_null <- rep(0, nrow(churn_test))

cm_train_null <- caret::confusionMatrix(as.factor(model_train_null), churn_train$Exited, positive = "1")
cm_train_null

cm_test_null <- caret::confusionMatrix(as.factor(model_test_null), churn_test$Exited, positive = "1")
cm_test_null
```

**QUESTION** A few questions:

* Are you surprised that the performance of the null model is almost identical on test and train? Why or why not?

> I am not surprised that the performance was so similar on both the test and training sets because we used the caret package to partition the data and this package maintains similar proportional values for `Exited` for each partition. Since we are just classifying all of the cases as 0, metrics such as accuracy, sensitivity, and specificity just depend on the split of 0s and 1s and since both partitions should have a similar split, these metrics should be very similar.

* Explain the sensitivity and specificity values. 

> Sensitivity is 0% on both sets because sensitivity measures the proportion of 1's correctly classified as a 1. Since all of the cases are classified as 0, no observations are correctly classified as 1, resulting in a 0% sensitity. Specificity is 100% on both sets because it measures the proportion of 0's correctly classified as a 0. Since, all the cases are classified as a 0, all of the 0 observations would be correctly classifed.

So, as we begin fitting more complicated models, remember that we need to
outperform the null model to make it worth it to use more complicated models.

Now I'm going to ask you to fit three models:

* a logistic regression model
* a simple decision tree
* a random forest

We covered all three of these modeling techniques in the class notes.

For each model type, you should:

* fit the model on the training data,
* assess the model's performance on the training data using the `confusionMatrix` function,
* use the model to make predictions on the test data,
* assess the model's performance on the test data using the `confusionMatrix` function,
* discuss the results

In your discussion of the results you should talk about things like:

* how accurate is the model in predicting on the test data
* is there evidence of overfitting?
* how does the model do in terms of other metrics like sensitivity and specificity
* other things you deem important.

### Fit logistic regression models

You'll start by creating a logistic regression model to predict `Exited`. Since there
are not that many variables, let's use all of them. Here's a code skeleton to help you get started:

**Hint**: There's an easy way to specify your model formula to include all of the predictor variables
without typing out all the variable names. 

```{r lr1_train}
# Fit model to training data
model_lr1 <- glm(Exited ~ .,
                data=churn_train, family=binomial(link="logit"))

# Convert fitted model values to fitted classes. Use 0.5 as the threshold for classifying a case as a 1.
class_train_lr1 <- as.factor((model_lr1$fitted.values > 0.5) * 1)

cm_train_lr1 <- confusionMatrix(class_train_lr1, churn_train$Exited, positive="1")
cm_train_lr1
```

Now, let's predict on test data.

```{r lr1_test}
# Make predictions on test data
pred_lr1 <- predict(model_lr1, newdata = churn_test, type = "response")

# Convert predicted values to fitted classes. Use 0.5 as the threshold for classifying a case as a 1.
class_test_lr1 <- as.factor((pred_lr1 > 0.5) * 1)

cm_test_lr1 <- confusionMatrix(class_test_lr1, churn_test$Exited, positive="1")
cm_test_lr1
```

**QUESTION** How did accuracy, sensitivity and specificity change when predicting on test data instead of the training data?

> Accuracy went from 83.63% on the training set to 83.2% on the test set. A slight decrease is expected in performance metrics on the test set so this was expected. An 83% accuracy on both sets is pretty good. However, sensitivity was quite poor with a 38.3% on the training set and a 37.5% on the test set. Specificity was excellent with about a 95% on both the training and test sets. Since there were no dramatic decreases in performance on the test sets, overfitting is not a concern. Overall, the model is good at predicting 0's but not at predicting 1's. 

Now change the threshold from 0.5 to 0.4 and create a new model using this new threshold. How does the sensitivity and specificity change as compared to our first logistic regression model? Explain why this happens?

```{r increase_sensitivity}
# Fit model to training data
model_lr2 <- glm(Exited ~ .,
                data=churn_train, family=binomial(link="logit"))

# Convert fitted model values to fitted classes. Use 0.4 as the threshold for classifying a case as a 1.
class_train_lr2 <- as.factor((model_lr2$fitted.values > 0.4) * 1)

# Create confusion matrix on training data
cm_train_lr2 <- confusionMatrix(class_train_lr2, churn_train$Exited, positive="1")
cm_train_lr2


# Make predictions on test data
pred_lr2 <- predict(model_lr2, newdata = churn_test, type = "response")

# Convert predicted values to fitted classes. Use 0.4 as the threshold for classifying a case as a 1.
class_test_lr2 <- as.factor((pred_lr2 > 0.4) * 1)

# Create confusion matrix on test data
cm_test_lr2 <- confusionMatrix(class_test_lr2, churn_test$Exited, positive="1")
cm_test_lr2

```

> Changing the threshold to 0.4 resulted in nearly the same accuracy on both sets (roughly 83%). The main change is seen in sensitivty and specificity. Sensitivity improved from around 38% when using a 0.5 threshold to about 50% when using a 0.4 threshold. Specificity, however, decreased from about 95% to around 92%. This increase in sensitivity was expected because in order to maximize sensitivity, one should lower the threshold. This is because more observations are classified as 1's since the probability only needs to be greater than 0.4 instead of 0.5.


### Fit simple decision tree model

Now create a simple decision tree model to predict `Exited`. Again, use all the variables.

```{r tree1_train}
model_tree1 <- rpart(Exited ~ ., data=churn_train)

class_train_tree1 <- predict(model_tree1, type="class")

cm_train_tree1 <- confusionMatrix(class_train_tree1, churn_train$Exited, positive="1")

cm_train_tree1
```

Create a plot of your decision tree.

```{r decision_tree_plot}
rpart.plot(model_tree1)
```

Explain the bottom left node of your tree. What conditions have to be true for a case to end up being classified by that node? What do those three numbers in the node mean? What does the color of the node mean?

> The left node would be true if age is less than 43 and the number of products is greater than or equal to 2 and less than 3. The first number in the node is the predicted class (either a 0 or a 1). The second number is the predicted probility of an observation in the node being a 1. The last number is the percentage of the total observations that are in the node. So for example, the bottom left node is classified as a 0, with a 0.04 probability of being a 1, and contains 43% of the data. The color of the node is blue for a 0 classification and green for a 1 classification. The more intense the color, the higher the purity. So, the bottom left node is a much more intense blue because it has a higher purity (only 0.04 probability of being a 1). However the first node on the right after the root node is much paler because there is a probability of 0.48 that a case is a 1, resulting in a lower purity. 

Now, let's predict on test data.

```{r tree1_test}
pred_tree1 <- predict(model_tree1, newdata = churn_test, type = "class")

cm_test_tree1 <- confusionMatrix(pred_tree1, churn_test$Exited, positive="1")

cm_test_tree1
```

**QUESTION** How does the performance of the decision tree compare to your logistic regression model? 

> The accuracy is similar on both the training and test sets for both the logistic regression model (83%) and the decision tree model (85%). Both models have good accuracy. Sensitivity is 45.4% on the training set and 43.6% on the test set for the decision tree. The logistic model had around a 38% accuracy when using a 0.5 threshold and around 50% when using a 0.4 threshold. So, the logistic model with a 0.4 threshold was best at predicting 1's. For the decision tree model, specificity is 96.2% on both sets indicating the model does an excellent job at classifying obserations as 0's. The logistic regression model had a specificity of around 95% with a 0.5 threshold and a 92% when using a 0.4 threshold. Overall, both models are good at predicting 0's but not at predicting 1's. The best model at predicting 1's is the logisitc regression model with a 0.4 threshold. 


## Fit random forest model

Finally, fit a random forest model.

```{r rf1_train}
model_rf1 <- randomForest(Exited ~ .,
                          data = churn_train,
                          mtry = 4, #rougly 1/3 of variables used
                          importance = TRUE,
                          na.action = na.omit)

class_train_rf1 <- predict(model_rf1, type="class")

cm_train_rf1 <- confusionMatrix(class_train_rf1, churn_train$Exited, positive="1")

cm_train_rf1
```

Now, let's predict on test data.

```{r rf1_test}
pred_rf1 <- predict(model_rf1, newdata = churn_test, type = "class")

cm_test_rf1 <- confusionMatrix(pred_rf1, churn_test$Exited, positive="1")

cm_test_rf1
```

**QUESTION** Summarize the performance of all three of your models (logistic, tree, random forest)? Is their evidence of overfitting in any of these model and what is your evidence for your answer? Add code chunks as needed.

```{r model_comparison}
# Logistic Regression (0.5 threshold) Metrics
sprintf("LR1 Accuracy: Fit = %.3f Pred = %.3f",
        cm_train_lr1$overall['Accuracy'], cm_test_lr1$overall['Accuracy'])
sprintf("LR1 Sensitivity: Fit = %.3f Pred = %.3f",
        cm_train_lr1$byClass['Sensitivity'], cm_test_lr1$byClass['Sensitivity'])
sprintf("LR1 Specificity: Fit = %.3f Pred = %.3f",
        cm_train_lr1$byClass['Specificity'], cm_test_lr1$byClass['Specificity'])

# Logistic Regression (0.4 threshold) Metrics
sprintf("LR2 Accuracy: Fit = %.3f Pred = %.3f",
        cm_train_lr2$overall['Accuracy'], cm_test_lr2$overall['Accuracy'])
sprintf("LR2 Sensitivity: Fit = %.3f Pred = %.3f",
        cm_train_lr2$byClass['Sensitivity'], cm_test_lr2$byClass['Sensitivity'])
sprintf("LR2 Specificity: Fit = %.3f Pred = %.3f",
        cm_train_lr2$byClass['Specificity'], cm_test_lr2$byClass['Specificity'])

# Decision Tree Metrics
sprintf("Tree1 Accuracy: Fit = %.3f Pred = %.3f",
        cm_train_tree1$overall['Accuracy'], cm_test_tree1$overall['Accuracy'])
sprintf("Tree1 Sensitivity: Fit = %.3f Pred = %.3f",
        cm_train_tree1$byClass['Sensitivity'], cm_test_tree1$byClass['Sensitivity'])
sprintf("Tree1 Specificity: Fit = %.3f Pred = %.3f",
        cm_train_tree1$byClass['Specificity'], cm_test_tree1$byClass['Specificity'])

# Random Forest Metrics
sprintf("RF1 Accuracy: Fit = %.3f Pred = %.3f",
        cm_train_rf1$overall['Accuracy'], cm_test_rf1$overall['Accuracy'])
sprintf("RF1 Sensitivity: Fit = %.3f Pred = %.3f",
        cm_train_rf1$byClass['Sensitivity'], cm_test_rf1$byClass['Sensitivity'])
sprintf("RF1 Specificity: Fit = %.3f Pred = %.3f",
        cm_train_rf1$byClass['Specificity'], cm_test_rf1$byClass['Specificity'])
```

> All three models have very similar accuracy ranging from rougly 83% to 86% with the Random Forest model being the most accurate. Sensitivity ranged from 0.376 on the test set of LR1 to 0.546 on the Random Forest model. Overall, all the models performed rather poorly at predicting 1's according to sensitivity. Specificity ranged from around 91% to about 96% indicating all the models were good at predicting 0's. Overall, the best model was the random forest model as it had comparable accuracy and specificity to the other models but had the best sensitivity. Additionally, overfitting is not a concern as all the models had similar performance on both the training and test sets with some only having a slight decrease on the test set, which is expected. 


**QUESTION** If you had to pick one to use in an actual financial environment, which model would you use and why? As a manager in charge of retention, what model performance metrics are you most interested in? What are the basic tradeoffs you see in terms of the initiatives you might undertake in response to such a model? For example, if you were really interested in reducing the number of customers exiting, maybe there are some things you might do to incent high risk (of exiting) customers to stay. Discuss.

> I would use the random forest model because it had a high accuracy (~86%) and high specificity (~95%) like the other models, but it also had the highest sensitivity (54.6% on the training and 53.6% on the test set). Although this sensitivity isn't great, it was the highest out of the four models. As shown from my analysis, the metrics I am most interested in is accuracy, sensitivity, and specificity. In this case, I am most interested in increasing sensitivity without severly compromising the models accuracy and specificity as sensitivity reflects the models ability to predict the cusomters who will exit the bank. As a manager of retention, being able to identify customers who may leave is important. Having a model that can identify customers at risk of leaving would allow me to implement incentive plans for these high risk customers. The incentives could include things like increased interest rates on their savings accounts, lowering interest rates on their credit cards, or just by reaching out and creating a personal connection with them.

**HACKER EXTRA**

Create a variable importance plot for your random forest to try to get a sense of which variables are most important in predicting customers likely to churn. Build another random forest using only the top 5 or so variables suggested by the importance plot. How does the performance of this reduced model compare to the original model?

```{r importance}
# Dataframe based on importance 
df_imp <- as.data.frame(model_rf1$importance) %>% 
  arrange(desc(MeanDecreaseGini))

# Row names are variables, convert to a column
df_imp <- tibble::rownames_to_column(df_imp, "variable")

# Plot importance
ggplot(data = df_imp) +
  geom_bar(aes(x = reorder(variable, MeanDecreaseAccuracy), 
               y = MeanDecreaseAccuracy), 
               stat = "identity") +
  coord_flip()
```

Fit to training set: 

```{r rf2_train}
model_rf2 <- randomForest(Exited ~ NumOfProducts + Age + Balance + IsActiveMember + Geography,
                          data = churn_train,
                          mtry = 2, #rougly 1/3 of variables used
                          importance = TRUE,
                          na.action = na.omit)

class_train_rf2 <- predict(model_rf2, type="class")

cm_train_rf2 <- confusionMatrix(class_train_rf2, churn_train$Exited, positive="1")

cm_train_rf2
```

Predict on test set:

```{r rf2_test}
pred_rf2 <- predict(model_rf2, newdata = churn_test, type = "class")

cm_test_rf2 <- confusionMatrix(pred_rf2, churn_test$Exited, positive="1")

cm_test_rf2
```

Compare Random Forest Models:

```{r rf_comparison}
# RF1 Model
sprintf("RF1 Accuracy: Fit = %.3f Pred = %.3f",
        cm_train_rf1$overall['Accuracy'], cm_test_rf1$overall['Accuracy'])
sprintf("RF1 Sensitivity: Fit = %.3f Pred = %.3f",
        cm_train_rf1$byClass['Sensitivity'], cm_test_rf1$byClass['Sensitivity'])
sprintf("RF1 Specificity: Fit = %.3f Pred = %.3f",
        cm_train_rf1$byClass['Specificity'], cm_test_rf1$byClass['Specificity'])

# RF2 Model
sprintf("RF2 Accuracy: Fit = %.3f Pred = %.3f",
        cm_train_rf2$overall['Accuracy'], cm_test_rf2$overall['Accuracy'])
sprintf("RF2 Sensitivity: Fit = %.3f Pred = %.3f",
        cm_train_rf2$byClass['Sensitivity'], cm_test_rf2$byClass['Sensitivity'])
sprintf("RF2 Specificity: Fit = %.3f Pred = %.3f",
        cm_train_rf2$byClass['Specificity'], cm_test_rf2$byClass['Specificity'])
```

**QUESTION** How does the performance of this reduced model compare to the original model?

> The accuracy is nearly identical for RF1 and this reduced model (aka RF2). The sensitivity is slightly worse on RF2 at about 50% on both sets while RF1 had a sensitivity of about 54% on both sets. Specificity was rougly the same around 95% for RF1 and 96% for RF2. Overall, the performance is about the same on both models.
